{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bad210-62d5-460a-bc80-2e961bada836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from keybert import KeyBERT\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcaf25cd-c78e-49c0-b13a-ab91495eee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemEval dataset\n",
    "sem_eval_ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "# Load Inspec dataset\n",
    "inspec_ds = load_dataset(\"midas/inspec\", \"generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380a4b74-4999-4d82-afc6-42c8b372026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KeyBERT model (default)...\n"
     ]
    }
   ],
   "source": [
    "def clean_sem_eval_sentence(text: str) -> str:\n",
    "    \"\"\"Remove entity XML tags from SemEval sentences.\"\"\"\n",
    "    return re.sub(r'</?e[12]>', '', text).strip()\n",
    "\n",
    "def prepare_inspec_text(tokens: List[str]) -> str:\n",
    "    \"\"\"Convert list of tokens to clean string for Inspec.\"\"\"\n",
    "    return ' '.join([t for t in tokens if not (t.startswith('-') and t.endswith('-'))])\n",
    "\n",
    "def extract_keyphrases(text: str, model: KeyBERT, top_n: int = 8) -> List[str]:\n",
    "    \"\"\"Extract keyphrases using KeyBERT with maxsum enabled.\"\"\"\n",
    "    keyphrases = model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 3),\n",
    "        stop_words='english',\n",
    "        top_n=top_n,\n",
    "        use_maxsum=True     # <-- Extension 1: improves diversity\n",
    "    )\n",
    "    return [kp[0] for kp in keyphrases]\n",
    "\n",
    "# Initialize KeyBERT with default embedding model\n",
    "print(\"Loading KeyBERT model (default)...\")\n",
    "model = KeyBERT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef55332-2a2d-4225-91c6-12039aeef5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_matches(true_phrases: List[str], extracted_phrases: List[str], partial_match: bool = True) -> Tuple[float, float, float]:\n",
    "    \"\"\"Calculate precision, recall, and F1 score with optional partial matching.\"\"\"\n",
    "    if partial_match:\n",
    "        matched_true = set()\n",
    "        matched_extracted = set()\n",
    "        for i, ext in enumerate(extracted_phrases):\n",
    "            for j, true in enumerate(true_phrases):\n",
    "                if ext.lower() in true.lower() or true.lower() in ext.lower():\n",
    "                    matched_extracted.add(i)\n",
    "                    matched_true.add(j)\n",
    "        matches = len(matched_true)\n",
    "    else:\n",
    "        matches = sum(1 for ext in extracted_phrases if any(ext.lower() == true.lower() for true in true_phrases))\n",
    "\n",
    "    precision = matches / len(extracted_phrases) if extracted_phrases else 0\n",
    "    recall = matches / len(true_phrases) if true_phrases else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00587fde-f133-4264-91f0-b8bc165b0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sem_eval(dataset, model, top_n=2):\n",
    "    results = []\n",
    "    metrics = []\n",
    "    for sample in tqdm(dataset):\n",
    "        text = clean_sem_eval_sentence(sample['sentence'])\n",
    "        true_entities = re.findall(r'<e[12]>(.*?)</e[12]>', sample['sentence'])\n",
    "        extracted = extract_keyphrases(text, model, top_n=top_n)\n",
    "\n",
    "        precision, recall, f1 = evaluate_matches(true_entities, extracted)\n",
    "        metrics.append((precision, recall, f1))\n",
    "\n",
    "        results.append({\n",
    "            'sentence': sample['sentence'],\n",
    "            'true_entities': true_entities,\n",
    "            'extracted_phrases': extracted,\n",
    "            'metrics': {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "        })\n",
    "    avg_metrics = {\n",
    "        'precision': np.mean([m[0] for m in metrics]),\n",
    "        'recall': np.mean([m[1] for m in metrics]),\n",
    "        'f1': np.mean([m[2] for m in metrics]),\n",
    "    }\n",
    "    return results, avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441de35d-59b5-4f5b-bea8-809a95fa4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inspec(dataset, model, top_n=8):\n",
    "    results = []\n",
    "    metrics = []\n",
    "    for sample in tqdm(dataset):\n",
    "        text = prepare_inspec_text(sample['document'])\n",
    "        true_keyphrases = sample['extractive_keyphrases']\n",
    "        extracted = extract_keyphrases(text, model, top_n=top_n)\n",
    "\n",
    "        precision, recall, f1 = evaluate_matches(true_keyphrases, extracted)\n",
    "        metrics.append((precision, recall, f1))\n",
    "\n",
    "        results.append({\n",
    "            'document': sample['document'],\n",
    "            'true_keyphrases': true_keyphrases,\n",
    "            'extracted_phrases': extracted,\n",
    "            'metrics': {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "        })\n",
    "    avg_metrics = {\n",
    "        'precision': np.mean([m[0] for m in metrics]),\n",
    "        'recall': np.mean([m[1] for m in metrics]),\n",
    "        'f1': np.mean([m[2] for m in metrics]),\n",
    "    }\n",
    "    return results, avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b21a695-413e-482a-b97f-6e4d0838633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SemEval-2010 train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc443dee90843d4880dc21e28b64d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SemEval-2010 test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ba1b7469ca4ede98fc59f8b69109a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SemEval-2010 Metrics:\n",
      "Train - Precision: 0.548, Recall: 0.548, F1: 0.548\n",
      "Test  - Precision: 0.544, Recall: 0.544, F1: 0.544\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing SemEval-2010 train dataset...\")\n",
    "sem_eval_train_results, sem_eval_train_metrics = process_sem_eval(sem_eval_ds['train'], model, top_n=2)\n",
    "\n",
    "print(\"Processing SemEval-2010 test dataset...\")\n",
    "sem_eval_test_results, sem_eval_test_metrics = process_sem_eval(sem_eval_ds['test'], model, top_n=2)\n",
    "\n",
    "print(\"\\nSemEval-2010 Metrics:\")\n",
    "print(f\"Train - Precision: {sem_eval_train_metrics['precision']:.3f}, Recall: {sem_eval_train_metrics['recall']:.3f}, F1: {sem_eval_train_metrics['f1']:.3f}\")\n",
    "print(f\"Test  - Precision: {sem_eval_test_metrics['precision']:.3f}, Recall: {sem_eval_test_metrics['recall']:.3f}, F1: {sem_eval_test_metrics['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae15d30e-33dc-4a2a-bb48-579fe5d14fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Inspec train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a220d645d5649c0b0116c11c1c148e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Inspec test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787df59a6fa945e79d0b6fbcc2cc390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspec Metrics:\n",
      "Train - Precision: 0.271, Recall: 0.403, F1: 0.300\n",
      "Test  - Precision: 0.286, Recall: 0.411, F1: 0.311\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Inspec train dataset...\")\n",
    "inspec_train_results, inspec_train_metrics = process_inspec(inspec_ds['train'], model, top_n=8)\n",
    "\n",
    "print(\"Processing Inspec test dataset...\")\n",
    "inspec_test_results, inspec_test_metrics = process_inspec(inspec_ds['test'], model, top_n=8)\n",
    "\n",
    "print(\"\\nInspec Metrics:\")\n",
    "print(f\"Train - Precision: {inspec_train_metrics['precision']:.3f}, Recall: {inspec_train_metrics['recall']:.3f}, F1: {inspec_train_metrics['f1']:.3f}\")\n",
    "print(f\"Test  - Precision: {inspec_test_metrics['precision']:.3f}, Recall: {inspec_test_metrics['recall']:.3f}, F1: {inspec_test_metrics['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a0898-82d9-42ee-b914-af604f6bbfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
