{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71eff8da-e04f-4c9c-bc64-5491793a320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ebteda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from keybert import KeyBERT\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861a32c4-f061-45d7-8697-57501e1c1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data structure:\n",
      "{'sentence': 'The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'relation': 3}\n",
      "\n",
      "Samples with their entities:\n",
      "\n",
      "1. Sentence: The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.\n",
      "   Entities: ['configuration', 'elements']\n",
      "\n",
      "2. Sentence: The <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.\n",
      "   Entities: ['child', 'cradle']\n",
      "\n",
      "3. Sentence: The <e1>author</e1> of a keygen uses a <e2>disassembler</e2> to look at the raw assembly code.\n",
      "   Entities: ['author', 'disassembler']\n",
      "\n",
      "4. Sentence: A misty <e1>ridge</e1> uprises from the <e2>surge</e2>.\n",
      "   Entities: ['ridge', 'surge']\n",
      "\n",
      "5. Sentence: The <e1>student</e1> <e2>association</e2> is the voice of the undergraduate student population of the State University of New York at Buffalo.\n",
      "   Entities: ['student', 'association']\n",
      "\n",
      "6. Sentence: This is the sprawling <e1>complex</e1> that is Peru's largest <e2>producer</e2> of silver.\n",
      "   Entities: ['complex', 'producer']\n",
      "\n",
      "7. Sentence: The current view is that the chronic <e1>inflammation</e1> in the distal part of the stomach caused by Helicobacter pylori <e2>infection</e2> results in an increased acid production from the non-infected upper corpus region of the stomach.\n",
      "   Entities: ['inflammation', 'infection']\n",
      "\n",
      "8. Sentence: <e1>People</e1> have been moving back into <e2>downtown</e2>.\n",
      "   Entities: ['People', 'downtown']\n",
      "\n",
      "9. Sentence: The <e1>lawsonite</e1> was contained in a <e2>platinum crucible</e2> and the counter-weight was a plastic crucible with metal pieces.\n",
      "   Entities: ['lawsonite', 'platinum crucible']\n",
      "\n",
      "10. Sentence: The solute was placed inside a beaker and 5 mL of the <e1>solvent</e1> was pipetted into a 25 mL glass <e2>flask</e2> for each trial.\n",
      "   Entities: ['solvent', 'flask']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "# Take first 10 samples for testing\n",
    "test_samples = list(ds['train'].select(range(10)))\n",
    "\n",
    "# Display first sample structure\n",
    "print(\"Sample data structure:\")\n",
    "print(test_samples[0])\n",
    "\n",
    "# Display all 10 sentences and their entities\n",
    "print(\"\\nSamples with their entities:\")\n",
    "for i, sample in enumerate(test_samples):\n",
    "    entities = re.findall(r'<e[12]>(.*?)</e[12]>', sample['sentence'])\n",
    "    print(f\"\\n{i+1}. Sentence: {sample['sentence']}\")\n",
    "    print(f\"   Entities: {entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824f0544-fb85-4ec1-9e26-4f0cbd9e5fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KeyBERT model...\n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(text: str) -> str:\n",
    "    \"\"\"Remove XML tags from text\"\"\"\n",
    "    return re.sub(r'</?e[12]>', '', text).strip()\n",
    "\n",
    "def extract_keyphrases(text: str, model: KeyBERT, top_n: int = 2) -> List[str]:\n",
    "    \"\"\"Extract keyphrases using KeyBERT\"\"\"\n",
    "    keyphrases = model.extract_keywords(text, \n",
    "                                      keyphrase_ngram_range=(1, 2),\n",
    "                                      stop_words='english',\n",
    "                                      top_n=top_n,\n",
    "                                      use_maxsum=True)\n",
    "    return [k[0] for k in keyphrases]\n",
    "\n",
    "# Initialize model\n",
    "print(\"Loading KeyBERT model...\")\n",
    "model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06936df-980d-40fb-9dff-a5abe5aeefbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Clean text: The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "True entities: ['configuration', 'elements']\n",
      "Extracted keyphrases: ['greatest', 'antenna elements']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Clean text: The child was carefully wrapped and bound into the cradle by means of a cord.\n",
      "True entities: ['child', 'cradle']\n",
      "Extracted keyphrases: ['means', 'child carefully']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "Clean text: The author of a keygen uses a disassembler to look at the raw assembly code.\n",
      "True entities: ['author', 'disassembler']\n",
      "Extracted keyphrases: ['look raw', 'assembly code']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "Clean text: A misty ridge uprises from the surge.\n",
      "True entities: ['ridge', 'surge']\n",
      "Extracted keyphrases: ['misty', 'uprises surge']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "Clean text: The student association is the voice of the undergraduate student population of the State University of New York at Buffalo.\n",
      "True entities: ['student', 'association']\n",
      "Extracted keyphrases: ['association voice', 'student population']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 6:\n",
      "Clean text: This is the sprawling complex that is Peru's largest producer of silver.\n",
      "True entities: ['complex', 'producer']\n",
      "Extracted keyphrases: ['sprawling complex', 'producer silver']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 7:\n",
      "Clean text: The current view is that the chronic inflammation in the distal part of the stomach caused by Helicobacter pylori infection results in an increased acid production from the non-infected upper corpus region of the stomach.\n",
      "True entities: ['inflammation', 'infection']\n",
      "Extracted keyphrases: ['acid production', 'inflammation distal']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 8:\n",
      "Clean text: People have been moving back into downtown.\n",
      "True entities: ['People', 'downtown']\n",
      "Extracted keyphrases: ['people', 'moving downtown']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 9:\n",
      "Clean text: The lawsonite was contained in a platinum crucible and the counter-weight was a plastic crucible with metal pieces.\n",
      "True entities: ['lawsonite', 'platinum crucible']\n",
      "Extracted keyphrases: ['weight plastic', 'crucible counter']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 10:\n",
      "Clean text: The solute was placed inside a beaker and 5 mL of the solvent was pipetted into a 25 mL glass flask for each trial.\n",
      "True entities: ['solvent', 'flask']\n",
      "Extracted keyphrases: ['placed inside', 'ml solvent']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process test samples\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # Get original sentence and true entities\n",
    "    sentence = sample['sentence']\n",
    "    true_entities = re.findall(r'<e[12]>(.*?)</e[12]>', sentence)\n",
    "    \n",
    "    # Clean sentence and extract keyphrases\n",
    "    clean_text = clean_sentence(sentence)\n",
    "    extracted_phrases = extract_keyphrases(clean_text, model)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Clean text: {clean_text}\")\n",
    "    print(f\"True entities: {true_entities}\")\n",
    "    print(f\"Extracted keyphrases: {extracted_phrases}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4882bae0-160f-478e-ba2c-408c5ecbd4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "True entities: ['configuration', 'elements']\n",
      "Extracted phrases: ['greatest', 'antenna elements']\n",
      "Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "Sample 2:\n",
      "True entities: ['child', 'cradle']\n",
      "Extracted phrases: ['means', 'child carefully']\n",
      "Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "Sample 3:\n",
      "True entities: ['author', 'disassembler']\n",
      "Extracted phrases: ['look raw', 'assembly code']\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Sample 4:\n",
      "True entities: ['ridge', 'surge']\n",
      "Extracted phrases: ['misty', 'uprises surge']\n",
      "Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "Sample 5:\n",
      "True entities: ['student', 'association']\n",
      "Extracted phrases: ['association voice', 'student population']\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00\n",
      "\n",
      "Sample 6:\n",
      "True entities: ['complex', 'producer']\n",
      "Extracted phrases: ['sprawling complex', 'producer silver']\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00\n",
      "\n",
      "Sample 7:\n",
      "True entities: ['inflammation', 'infection']\n",
      "Extracted phrases: ['acid production', 'inflammation distal']\n",
      "Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "Sample 8:\n",
      "True entities: ['People', 'downtown']\n",
      "Extracted phrases: ['people', 'moving downtown']\n",
      "Precision: 1.00, Recall: 1.00, F1: 1.00\n",
      "\n",
      "Sample 9:\n",
      "True entities: ['lawsonite', 'platinum crucible']\n",
      "Extracted phrases: ['weight plastic', 'crucible counter']\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "\n",
      "Sample 10:\n",
      "True entities: ['solvent', 'flask']\n",
      "Extracted phrases: ['placed inside', 'ml solvent']\n",
      "Precision: 0.50, Recall: 0.50, F1: 0.50\n",
      "\n",
      "Average Metrics:\n",
      "Precision: 0.55\n",
      "Recall: 0.55\n",
      "F1 Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "def evaluate_matches(true_entities: List[str], extracted_phrases: List[str], partial_match: bool = True) -> Tuple[float, float, float]:\n",
    "    \"\"\"Calculate precision, recall, and F1 score\"\"\"\n",
    "    if partial_match:\n",
    "        # Count each match only once by tracking which true entities have been matched\n",
    "        matched_true = set()\n",
    "        matched_extracted = set()\n",
    "        \n",
    "        for i, ext in enumerate(extracted_phrases):\n",
    "            for j, gold in enumerate(true_entities):\n",
    "                if (ext.lower() in gold.lower() or gold.lower() in ext.lower()):\n",
    "                    matched_extracted.add(i)\n",
    "                    matched_true.add(j)\n",
    "        \n",
    "        matches = len(matched_true)  # Count unique matches\n",
    "    else:\n",
    "        matches = sum(1 for ext in extracted_phrases \n",
    "                     if any(ext.lower() == gold.lower() for gold in true_entities))\n",
    "    \n",
    "    precision = matches / len(extracted_phrases) if extracted_phrases else 0\n",
    "    recall = matches / len(true_entities) if true_entities else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Evaluate all samples\n",
    "all_metrics = []\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # Get entities and extract keyphrases\n",
    "    true_entities = re.findall(r'<e[12]>(.*?)</e[12]>', sample['sentence'])\n",
    "    clean_text = clean_sentence(sample['sentence'])\n",
    "    extracted_phrases = extract_keyphrases(clean_text, model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1 = evaluate_matches(true_entities, extracted_phrases)\n",
    "    all_metrics.append((precision, recall, f1))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"True entities: {true_entities}\")\n",
    "    print(f\"Extracted phrases: {extracted_phrases}\")\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_precision = np.mean([m[0] for m in all_metrics])\n",
    "avg_recall = np.mean([m[1] for m in all_metrics])\n",
    "avg_f1 = np.mean([m[2] for m in all_metrics])\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f\"Precision: {avg_precision:.2f}\")\n",
    "print(f\"Recall: {avg_recall:.2f}\")\n",
    "print(f\"F1 Score: {avg_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4c61d6-93d2-4b86-ae4d-bd6bf7ae1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_in_batches(dataset, model, batch_size=200, save_every=1000):\n",
    "    \"\"\"\n",
    "    Process the dataset in batches with progress monitoring and periodic saving\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    all_metrics = []\n",
    "    all_results = []\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=total_samples, desc=\"Processing samples\")\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        # Get batch\n",
    "        batch = dataset.select(range(i, min(i + batch_size, total_samples)))\n",
    "        batch_metrics = []\n",
    "        batch_results = []\n",
    "        \n",
    "        # Process each sample in batch\n",
    "        for sample in batch:\n",
    "            true_entities = re.findall(r'<e[12]>(.*?)</e[12]>', sample['sentence'])\n",
    "            clean_text = clean_sentence(sample['sentence'])\n",
    "            extracted_phrases = extract_keyphrases(clean_text, model)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision, recall, f1 = evaluate_matches(true_entities, extracted_phrases)\n",
    "            batch_metrics.append((precision, recall, f1))\n",
    "            \n",
    "            # Store detailed results\n",
    "            batch_results.append({\n",
    "                'sentence': sample['sentence'],\n",
    "                'true_entities': true_entities,\n",
    "                'extracted_phrases': extracted_phrases,\n",
    "                'metrics': {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "            })\n",
    "        \n",
    "        # Update main lists\n",
    "        all_metrics.extend(batch_metrics)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(len(batch))\n",
    "        \n",
    "        # Save periodically\n",
    "        if (i + batch_size) % save_every == 0:\n",
    "            save_results(all_results, all_metrics, i + batch_size)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate final averages\n",
    "    avg_metrics = calculate_average_metrics(all_metrics)\n",
    "    \n",
    "    # Print time taken\n",
    "    time_taken = time.time() - start_time\n",
    "    print(f\"\\nTotal time taken: {time_taken:.2f} seconds\")\n",
    "    \n",
    "    return all_results, avg_metrics\n",
    "\n",
    "def save_results(results, metrics, samples_processed):\n",
    "    \"\"\"Save results to file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"keyphrase_results_{samples_processed}_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'results': results,\n",
    "            'metrics': metrics,\n",
    "            'samples_processed': samples_processed\n",
    "        }, f)\n",
    "    print(f\"\\nSaved results to {filename}\")\n",
    "\n",
    "def calculate_average_metrics(metrics):\n",
    "    \"\"\"Calculate average metrics from list of (precision, recall, f1) tuples\"\"\"\n",
    "    avg_precision = np.mean([m[0] for m in metrics])\n",
    "    avg_recall = np.mean([m[1] for m in metrics])\n",
    "    avg_f1 = np.mean([m[2] for m in metrics])\n",
    "    \n",
    "    return {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1b3a1f-a2ed-446c-82c7-bd5d964d7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea232cae08a4405d909859009546a446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing samples:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results to keyphrase_results_1000_20241209_121608.pkl\n",
      "\n",
      "Saved results to keyphrase_results_2000_20241209_121712.pkl\n",
      "\n",
      "Saved results to keyphrase_results_3000_20241209_121817.pkl\n",
      "\n",
      "Saved results to keyphrase_results_4000_20241209_121923.pkl\n",
      "\n",
      "Saved results to keyphrase_results_5000_20241209_122030.pkl\n",
      "\n",
      "Saved results to keyphrase_results_6000_20241209_122136.pkl\n",
      "\n",
      "Saved results to keyphrase_results_7000_20241209_122241.pkl\n",
      "\n",
      "Saved results to keyphrase_results_8000_20241209_122350.pkl\n",
      "\n",
      "Total time taken: 522.57 seconds\n",
      "\n",
      "Final Average Metrics:\n",
      "Precision: 0.425\n",
      "Recall: 0.425\n",
      "F1 Score: 0.425\n"
     ]
    }
   ],
   "source": [
    "# Process the full training dataset\n",
    "print(\"Starting full dataset processing...\")\n",
    "results, avg_metrics = process_dataset_in_batches(ds['train'], model)\n",
    "\n",
    "print(\"\\nFinal Average Metrics:\")\n",
    "print(f\"Precision: {avg_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {avg_metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score: {avg_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18a89c9-f57a-464e-bac7-70cea70fbe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8adcb3de20e400882f36c7703b1a0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing samples:   0%|          | 0/2717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results to keyphrase_results_1000_20241209_122457.pkl\n",
      "\n",
      "Saved results to keyphrase_results_2000_20241209_122603.pkl\n",
      "\n",
      "Total time taken: 180.60 seconds\n",
      "\n",
      "Test Set Metrics:\n",
      "Precision: 0.423\n",
      "Recall: 0.423\n",
      "F1 Score: 0.423\n"
     ]
    }
   ],
   "source": [
    "# Process the test dataset\n",
    "print(\"Starting test dataset processing...\")\n",
    "test_results, test_metrics = process_dataset_in_batches(ds['test'], model)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Precision: {test_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589beac5-dc55-488c-ba62-2c8066d51ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data structure:\n",
      "{'id': 1001, 'document': ['A', 'conflict', 'between', 'language', 'and', 'atomistic', 'information', 'Fred', 'Dretske', 'and', 'Jerry', 'Fodor', 'are', 'responsible', 'for', 'popularizing', 'three', 'well-known', 'theses', 'in', 'contemporary', 'philosophy', 'of', 'mind', ':', 'the', 'thesis', 'of', 'Information-Based', 'Semantics', '-LRB-', 'IBS', '-RRB-', ',', 'the', 'thesis', 'of', 'Content', 'Atomism', '-LRB-', 'Atomism', '-RRB-', 'and', 'the', 'thesis', 'of', 'the', 'Language', 'of', 'Thought', '-LRB-', 'LOT', '-RRB-', '.', 'LOT', 'concerns', 'the', 'semantically', 'relevant', 'structure', 'of', 'representations', 'involved', 'in', 'cognitive', 'states', 'such', 'as', 'beliefs', 'and', 'desires', '.', 'It', 'maintains', 'that', 'all', 'such', 'representations', 'must', 'have', 'syntactic', 'structures', 'mirroring', 'the', 'structure', 'of', 'their', 'contents', '.', 'IBS', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'relations', 'that', 'connect', 'cognitive', 'representations', 'and', 'their', 'parts', 'to', 'their', 'contents', '-LRB-', 'semantic', 'relations', '-RRB-', '.', 'It', 'holds', 'that', 'these', 'relations', 'supervene', 'solely', 'on', 'relations', 'of', 'the', 'kind', 'that', 'support', 'information', 'content', ',', 'perhaps', 'with', 'some', 'help', 'from', 'logical', 'principles', 'of', 'combination', '.', 'Atomism', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'content', 'of', 'simple', 'symbols', '.', 'It', 'holds', 'that', 'each', 'substantive', 'simple', 'symbol', 'possesses', 'its', 'content', 'independently', 'of', 'all', 'other', 'symbols', 'in', 'the', 'representational', 'system', '.', 'I', 'argue', 'that', 'Dretske', \"'s\", 'and', 'Fodor', \"'s\", 'theories', 'are', 'false', 'and', 'that', 'their', 'falsehood', 'results', 'from', 'a', 'conflict', 'IBS', 'and', 'Atomism', ',', 'on', 'the', 'one', 'hand', ',', 'and', 'LOT', ',', 'on', 'the', 'other'], 'extractive_keyphrases': ['philosophy of mind', 'content atomism', 'ibs', 'language of thought', 'lot', 'cognitive states', 'beliefs', 'desires'], 'abstractive_keyphrases': ['information-based semantics']}\n",
      "\n",
      "First 10 samples with their keyphrases:\n",
      "\n",
      "1. Document preview: A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in...\n",
      "   Extractive keyphrases: ['philosophy of mind', 'content atomism', 'ibs', 'language of thought', 'lot', 'cognitive states', 'beliefs', 'desires']\n",
      "\n",
      "2. Document preview: Selective representing and world-making We discuss the thesis of selective representing-the idea that the contents of the mental representations had...\n",
      "   Extractive keyphrases: ['selective representing', 'mental representations', 'organisms', 'realism', 'cognitive profiles']\n",
      "\n",
      "3. Document preview: Does classicism explain universality ? Arguments against a pure classical component of mind One of the hallmarks of human cognition...\n",
      "   Extractive keyphrases: ['classicism', 'universality', 'classical component of mind', 'human cognition', 'universal generalization', 'connectionist models', 'classical symbol systems', 'causal explanations', 'mental processes']\n",
      "\n",
      "4. Document preview: Separate accounts go mainstream -LSB- investment -RSB- New entrants are shaking up the separate-account industry by supplying Web-based platforms that...\n",
      "   Extractive keyphrases: ['independent money managers', 'investment']\n",
      "\n",
      "5. Document preview: Evolving receptive-field controllers for mobile robots The use of evolutionary methods to generate controllers for real-world autonomous agents has attracted...\n",
      "   Extractive keyphrases: ['mobile robots', 'evolutionary methods', 'evolution strategies', 'simple braitenberg vehicles', 'nonlinear interactions', 'complex behavior']\n",
      "\n",
      "6. Document preview: A scalable model of cerebellar adaptive timing and sequencing : the recurrent slide and latch -LRB- RSL -RRB- model From...\n",
      "   Extractive keyphrases: ['scalable model', 'cerebellar adaptive timing', 'neural network theory', 'mammalian cerebellum', 'granule cell stage', 'sparse expansive recoding', 'distributed representation']\n",
      "\n",
      "7. Document preview: A suggestion of fractional-order controller for flexible spacecraft attitude control A controller design method for flexible spacecraft attitude control is...\n",
      "   Extractive keyphrases: ['flexible spacecraft attitude control', 'partial differential equation', 'internal damping', 'frequency response', 'phase stabilization control', 'amplitude stabilization', 'damping ratio']\n",
      "\n",
      "8. Document preview: Extracting straight road structure in urban environments using IKONOS satellite imagery We discuss a fully automatic technique for extracting roads...\n",
      "   Extractive keyphrases: ['straight road structure', 'urban environments', 'ikonos satellite imagery', 'fully automatic technique', 'vegetation mask', 'texture', 'panchromatic ikonos data', 'road pixels', 'road network components', 'san diego']\n",
      "\n",
      "9. Document preview: Nonlinear control of a shape memory alloy actuated manipulator This paper presents a nonlinear , robust control algorithm for accurate...\n",
      "   Extractive keyphrases: ['shape memory alloy', 'manipulator', 'open loop', 'closed loop', 'variable structure control', 'control gain switching', 'nonlinear control', 'stabilization', 'tracking', 'positioning', 'nonlinear dynamics']\n",
      "\n",
      "10. Document preview: Lob 's theorem as a limitation on mechanism We argue that Lob 's Theorem implies a limitation on mechanism ....\n",
      "   Extractive keyphrases: ['limitation on mechanism', 'epistemic authority', 'formal system']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset 2\n",
    "ds_inspec = load_dataset(\"midas/inspec\", \"generation\")\n",
    "\n",
    "# We'll only use train and test splits (ignoring validation)\n",
    "train_samples = ds_inspec['train']\n",
    "test_samples = ds_inspec['test']\n",
    "\n",
    "# Take first 10 samples for testing (from train set)\n",
    "test_samples = list(train_samples.select(range(10)))\n",
    "\n",
    "# Display first sample structure\n",
    "print(\"Sample data structure:\")\n",
    "print(test_samples[0])\n",
    "\n",
    "# Display all 10 samples\n",
    "print(\"\\nFirst 10 samples with their keyphrases:\")\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\n{i+1}. Document preview: {' '.join(sample['document'][:20])}...\")\n",
    "    print(f\"   Extractive keyphrases: {sample['extractive_keyphrases']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c224d18-0f97-4662-b4c6-c9b58c240668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KeyBERT model...\n"
     ]
    }
   ],
   "source": [
    "def prepare_text(tokens: List[str]) -> str:\n",
    "    \"\"\"Convert token list to string, removing special tokens\"\"\"\n",
    "    cleaned_tokens = [t for t in tokens if not (t.startswith('-') and t.endswith('-'))]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "def extract_keyphrases(text: str, model: KeyBERT, top_n: int = 8) -> List[str]:\n",
    "    \"\"\"Extract keyphrases using KeyBERT\"\"\"\n",
    "    try:\n",
    "        keyphrases = model.extract_keywords(text, \n",
    "                                          keyphrase_ngram_range=(1, 3),\n",
    "                                          stop_words='english',\n",
    "                                          top_n=top_n)\n",
    "    except AttributeError:\n",
    "        # For newer scikit-learn versions\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        model.model = sentence_model\n",
    "        keyphrases = model.extract_keywords(text, \n",
    "                                          keyphrase_ngram_range=(1, 3),\n",
    "                                          stop_words='english',\n",
    "                                          top_n=top_n)\n",
    "    \n",
    "    return [k[0] for k in keyphrases]\n",
    "\n",
    "# Initialize model\n",
    "print(\"Loading KeyBERT model...\")\n",
    "model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4aa0588-e9e0-46fa-840f-f53630d6387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Document preview: A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics...\n",
      "True keyphrases: ['philosophy of mind', 'content atomism', 'ibs', 'language of thought', 'lot', 'cognitive states', 'beliefs', 'desires']\n",
      "Extracted keyphrases: ['language atomistic information', 'atomistic information', 'language atomistic', 'atomism thesis language', 'content atomism atomism', 'conflict language atomistic', 'semantics ibs thesis', 'atomistic information fred']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Document preview: Selective representing and world-making We discuss the thesis of selective representing-the idea that the contents of the mental representations had by organisms are highly constrained by the biological niches within...\n",
      "True keyphrases: ['selective representing', 'mental representations', 'organisms', 'realism', 'cognitive profiles']\n",
      "Extracted keyphrases: ['mental representations organisms', 'selective representing realism', 'selective representing world', 'representations organisms', 'representations organisms highly', 'realist conception mind', 'selective representing idea', 'mental representations']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "Document preview: Does classicism explain universality ? Arguments against a pure classical component of mind One of the hallmarks of human cognition is the capacity to generalize over arbitrary constituents . Marcus...\n",
      "True keyphrases: ['classicism', 'universality', 'classical component of mind', 'human cognition', 'universal generalization', 'connectionist models', 'classical symbol systems', 'causal explanations', 'mental processes']\n",
      "Extracted keyphrases: ['classicism explain universality', 'universality explained classical', 'universality explained', 'universality best explained', 'universality problem classicism', 'generalization universality', 'universality supported connectionist', 'does classicism explain']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "Document preview: Separate accounts go mainstream -LSB- investment -RSB- New entrants are shaking up the separate-account industry by supplying Web-based platforms that give advisers the tools to pick independent money managers...\n",
      "True keyphrases: ['independent money managers', 'investment']\n",
      "Extracted keyphrases: ['separate account industry', 'account industry', 'accounts mainstream investment', 'separate accounts mainstream', 'independent money managers', 'account industry supplying', 'separate accounts', 'separate account']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "Document preview: Evolving receptive-field controllers for mobile robots The use of evolutionary methods to generate controllers for real-world autonomous agents has attracted attention . Most of the pertinent research has employed genetic...\n",
      "True keyphrases: ['mobile robots', 'evolutionary methods', 'evolution strategies', 'simple braitenberg vehicles', 'nonlinear interactions', 'complex behavior']\n",
      "Extracted keyphrases: ['evolving receptive field', 'receptive field controllers', 'evolving receptive', 'robots use evolutionary', 'evolution strategies generation', 'evolution strategy genetic', 'evolution strategies', 'evolutionary method evolution']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the keyphrase extraction on first few samples\n",
    "for i, sample in enumerate(test_samples[:5]):  # Test first 5 samples\n",
    "    # Prepare text\n",
    "    text = prepare_text(sample['document'])\n",
    "    \n",
    "    # Extract keyphrases\n",
    "    extracted_phrases = extract_keyphrases(text, model)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Document preview: {' '.join(sample['document'][:30])}...\")\n",
    "    print(f\"True keyphrases: {sample['extractive_keyphrases']}\")\n",
    "    print(f\"Extracted keyphrases: {extracted_phrases}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "944eafd3-0e25-4d07-acbc-4c52cdd9b810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "True keyphrases: ['philosophy of mind', 'content atomism', 'ibs', 'language of thought', 'lot', 'cognitive states', 'beliefs', 'desires']\n",
      "Extracted phrases: ['language atomistic information', 'atomistic information', 'language atomistic', 'atomism thesis language', 'content atomism atomism', 'conflict language atomistic', 'semantics ibs thesis', 'atomistic information fred']\n",
      "Precision: 0.25, Recall: 0.25, F1: 0.25\n",
      "\n",
      "Sample 2:\n",
      "True keyphrases: ['selective representing', 'mental representations', 'organisms', 'realism', 'cognitive profiles']\n",
      "Extracted phrases: ['mental representations organisms', 'selective representing realism', 'selective representing world', 'representations organisms', 'representations organisms highly', 'realist conception mind', 'selective representing idea', 'mental representations']\n",
      "Precision: 0.50, Recall: 0.80, F1: 0.62\n",
      "\n",
      "Sample 3:\n",
      "True keyphrases: ['classicism', 'universality', 'classical component of mind', 'human cognition', 'universal generalization', 'connectionist models', 'classical symbol systems', 'causal explanations', 'mental processes']\n",
      "Extracted phrases: ['classicism explain universality', 'universality explained classical', 'universality explained', 'universality best explained', 'universality problem classicism', 'generalization universality', 'universality supported connectionist', 'does classicism explain']\n",
      "Precision: 0.25, Recall: 0.22, F1: 0.24\n",
      "\n",
      "Sample 4:\n",
      "True keyphrases: ['independent money managers', 'investment']\n",
      "Extracted phrases: ['separate account industry', 'account industry', 'accounts mainstream investment', 'separate accounts mainstream', 'independent money managers', 'account industry supplying', 'separate accounts', 'separate account']\n",
      "Precision: 0.25, Recall: 1.00, F1: 0.40\n",
      "\n",
      "Sample 5:\n",
      "True keyphrases: ['mobile robots', 'evolutionary methods', 'evolution strategies', 'simple braitenberg vehicles', 'nonlinear interactions', 'complex behavior']\n",
      "Extracted phrases: ['evolving receptive field', 'receptive field controllers', 'evolving receptive', 'robots use evolutionary', 'evolution strategies generation', 'evolution strategy genetic', 'evolution strategies', 'evolutionary method evolution']\n",
      "Precision: 0.12, Recall: 0.17, F1: 0.14\n",
      "\n",
      "Sample 6:\n",
      "True keyphrases: ['scalable model', 'cerebellar adaptive timing', 'neural network theory', 'mammalian cerebellum', 'granule cell stage', 'sparse expansive recoding', 'distributed representation']\n",
      "Extracted phrases: ['cerebellar adaptive timing', 'model cerebellar adaptive', 'models cerebellar adaptive', 'scalable model cerebellar', 'cerebellar adaptive', 'cerebellar cortex performs', 'models cerebellar', 'cerebellar outputs movement']\n",
      "Precision: 0.25, Recall: 0.29, F1: 0.27\n",
      "\n",
      "Sample 7:\n",
      "True keyphrases: ['flexible spacecraft attitude control', 'partial differential equation', 'internal damping', 'frequency response', 'phase stabilization control', 'amplitude stabilization', 'damping ratio']\n",
      "Extracted phrases: ['controller flexible spacecraft', 'fractional order controller', 'flexible spacecraft attitude', 'spacecraft attitude control', 'method flexible spacecraft', 'flexible spacecraft', 'spacecraft attitude', 'attitude control controller']\n",
      "Precision: 0.12, Recall: 0.14, F1: 0.13\n",
      "\n",
      "Sample 8:\n",
      "True keyphrases: ['straight road structure', 'urban environments', 'ikonos satellite imagery', 'fully automatic technique', 'vegetation mask', 'texture', 'panchromatic ikonos data', 'road pixels', 'road network components', 'san diego']\n",
      "Extracted phrases: ['distinguish road pixels', 'technique extracting roads', 'road pixels', 'underlying road structure', 'extracting roads', 'distinguish road', 'road pixels individual', 'road network scene']\n",
      "Precision: 0.12, Recall: 0.10, F1: 0.11\n",
      "\n",
      "Sample 9:\n",
      "True keyphrases: ['shape memory alloy', 'manipulator', 'open loop', 'closed loop', 'variable structure control', 'control gain switching', 'nonlinear control', 'stabilization', 'tracking', 'positioning', 'nonlinear dynamics']\n",
      "Extracted phrases: ['manipulator actuated shape', 'sma actuated manipulator', 'alloy actuated manipulator', 'nonlinear dynamics manipulator', 'actuated shape memory', 'memory alloy actuated', 'nonlinear control shape', 'actuated manipulator']\n",
      "Precision: 0.38, Recall: 0.27, F1: 0.32\n",
      "\n",
      "Sample 10:\n",
      "True keyphrases: ['limitation on mechanism', 'epistemic authority', 'formal system']\n",
      "Extracted phrases: ['epistemic agents', 'epistemic authority', 'mechanism argue lob', 'epistemic authority particular', 'mechanical used epistemic', 'observer belief', 'used epistemic authority', 'epistemic']\n",
      "Precision: 0.12, Recall: 0.33, F1: 0.18\n",
      "\n",
      "Average Metrics:\n",
      "Precision: 0.24\n",
      "Recall: 0.36\n",
      "F1 Score: 0.27\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define evaluation metrics\n",
    "def evaluate_matches(true_keyphrases: List[str], extracted_phrases: List[str], partial_match: bool = True) -> Tuple[float, float, float]:\n",
    "    \"\"\"Calculate precision, recall, and F1 score\"\"\"\n",
    "    if partial_match:\n",
    "        # Count each match only once by tracking which true keyphrases have been matched\n",
    "        matched_true = set()\n",
    "        matched_extracted = set()\n",
    "        \n",
    "        for i, ext in enumerate(extracted_phrases):\n",
    "            for j, gold in enumerate(true_keyphrases):\n",
    "                if (ext.lower() in gold.lower() or gold.lower() in ext.lower()):\n",
    "                    matched_extracted.add(i)\n",
    "                    matched_true.add(j)\n",
    "        \n",
    "        matches = len(matched_true)  # Count unique matches\n",
    "    else:\n",
    "        matches = sum(1 for ext in extracted_phrases \n",
    "                     if any(ext.lower() == gold.lower() for gold in true_keyphrases))\n",
    "    \n",
    "    precision = matches / len(extracted_phrases) if extracted_phrases else 0\n",
    "    recall = matches / len(true_keyphrases) if true_keyphrases else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Evaluate sample results\n",
    "all_metrics = []\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # Prepare text and extract keyphrases\n",
    "    text = prepare_text(sample['document'])\n",
    "    extracted_phrases = extract_keyphrases(text, model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1 = evaluate_matches(sample['extractive_keyphrases'], extracted_phrases)\n",
    "    all_metrics.append((precision, recall, f1))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"True keyphrases: {sample['extractive_keyphrases']}\")\n",
    "    print(f\"Extracted phrases: {extracted_phrases}\")\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_precision = np.mean([m[0] for m in all_metrics])\n",
    "avg_recall = np.mean([m[1] for m in all_metrics])\n",
    "avg_f1 = np.mean([m[2] for m in all_metrics])\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f\"Precision: {avg_precision:.2f}\")\n",
    "print(f\"Recall: {avg_recall:.2f}\")\n",
    "print(f\"F1 Score: {avg_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1305fdbd-7731-46f3-9e74-a4dcc0fe1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_in_batches(dataset, model, batch_size=200, save_every=1000):\n",
    "    \"\"\"Process the dataset in batches with progress monitoring and periodic saving\"\"\"\n",
    "    start_time = time.time()\n",
    "    all_metrics = []\n",
    "    all_results = []\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=total_samples, desc=\"Processing samples\")\n",
    "    \n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        # Get batch\n",
    "        batch = dataset.select(range(i, min(i + batch_size, total_samples)))\n",
    "        batch_metrics = []\n",
    "        batch_results = []\n",
    "        \n",
    "        # Process each sample in batch\n",
    "        for sample in batch:\n",
    "            # Prepare text and extract keyphrases\n",
    "            text = prepare_text(sample['document'])\n",
    "            extracted_phrases = extract_keyphrases(text, model)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision, recall, f1 = evaluate_matches(sample['extractive_keyphrases'], \n",
    "                                                   extracted_phrases)\n",
    "            batch_metrics.append((precision, recall, f1))\n",
    "            \n",
    "            # Store detailed results\n",
    "            batch_results.append({\n",
    "                'document': sample['document'],\n",
    "                'true_keyphrases': sample['extractive_keyphrases'],\n",
    "                'extracted_phrases': extracted_phrases,\n",
    "                'metrics': {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "            })\n",
    "        \n",
    "        # Update main lists\n",
    "        all_metrics.extend(batch_metrics)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(len(batch))\n",
    "        \n",
    "        # Save periodically\n",
    "        if (i + batch_size) % save_every == 0:\n",
    "            save_results(all_results, all_metrics, i + batch_size)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate final averages\n",
    "    avg_metrics = calculate_average_metrics(all_metrics)\n",
    "    \n",
    "    # Print time taken\n",
    "    time_taken = time.time() - start_time\n",
    "    print(f\"\\nTotal time taken: {time_taken:.2f} seconds\")\n",
    "    \n",
    "    return all_results, avg_metrics\n",
    "\n",
    "def save_results(results, metrics, samples_processed):\n",
    "    \"\"\"Save results to file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"inspec_results_{samples_processed}_{timestamp}.pkl\"\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'results': results,\n",
    "            'metrics': metrics,\n",
    "            'samples_processed': samples_processed\n",
    "        }, f)\n",
    "    print(f\"\\nSaved results to {filename}\")\n",
    "\n",
    "def calculate_average_metrics(metrics):\n",
    "    \"\"\"Calculate average metrics\"\"\"\n",
    "    avg_precision = np.mean([m[0] for m in metrics])\n",
    "    avg_recall = np.mean([m[1] for m in metrics])\n",
    "    avg_f1 = np.mean([m[2] for m in metrics])\n",
    "    \n",
    "    return {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e91f86-6e06-4374-9b01-ba181282e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75805a95d13249e79818fd65208f4886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing samples:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results to inspec_results_1000_20241209_123504.pkl\n",
      "\n",
      "Total time taken: 482.16 seconds\n",
      "\n",
      "Training Set Final Metrics:\n",
      "Precision: 0.216\n",
      "Recall: 0.335\n",
      "F1 Score: 0.241\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Process full training dataset\n",
    "print(\"Starting full training dataset processing...\")\n",
    "train_results, train_metrics = process_dataset_in_batches(ds_inspec['train'], model)\n",
    "print(\"\\nTraining Set Final Metrics:\")\n",
    "print(f\"Precision: {train_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {train_metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score: {train_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21ce93f-0cd6-433a-8751-91661d53548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a032e5ea1d749f793faec56df9f97c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing samples:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time taken: 233.89 seconds\n",
      "\n",
      "Test Set Metrics:\n",
      "Precision: 0.216\n",
      "Recall: 0.321\n",
      "F1 Score: 0.238\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Process test dataset\n",
    "print(\"Starting test dataset processing...\")\n",
    "test_results, test_metrics = process_dataset_in_batches(ds_inspec['test'], model)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Precision: {test_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd0ac306-0f7b-4152-aeab-3dffbf6c8cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Metrics:\n",
      "| Dataset             | Split   |   Precision |   Recall |   F1 Score |\n",
      "|:--------------------|:--------|------------:|---------:|-----------:|\n",
      "| SemEval-2010 Task 8 | Train   |       0.425 |    0.425 |      0.425 |\n",
      "| SemEval-2010 Task 8 | Test    |       0.216 |    0.321 |      0.238 |\n",
      "| Inspec              | Train   |       0.216 |    0.335 |      0.241 |\n",
      "| Inspec              | Test    |       0.216 |    0.321 |      0.238 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Collecting results from our experiments\n",
    "def create_comparison_table(sem_eval_train, sem_eval_test, inspec_train, inspec_test):\n",
    "    data = [\n",
    "        ['SemEval-2010 Task 8', 'Train', \n",
    "         sem_eval_train['precision'], \n",
    "         sem_eval_train['recall'], \n",
    "         sem_eval_train['f1']],\n",
    "        ['SemEval-2010 Task 8', 'Test', \n",
    "         sem_eval_test['precision'], \n",
    "         sem_eval_test['recall'], \n",
    "         sem_eval_test['f1']],\n",
    "        ['Inspec', 'Train', \n",
    "         inspec_train['precision'], \n",
    "         inspec_train['recall'], \n",
    "         inspec_train['f1']],\n",
    "        ['Inspec', 'Test', \n",
    "         inspec_test['precision'], \n",
    "         inspec_test['recall'], \n",
    "         inspec_test['f1']]\n",
    "    ]\n",
    "    \n",
    "    headers = ['Dataset', 'Split', 'Precision', 'Recall', 'F1 Score']\n",
    "    print(\"\\nBaseline Metrics:\")\n",
    "    print(tabulate(data, headers=headers, tablefmt='pipe', floatfmt='.3f'))\n",
    "\n",
    "# Use our actual metrics from previous runs\n",
    "create_comparison_table(\n",
    "    sem_eval_train=avg_metrics,      # from SemEval train processing\n",
    "    sem_eval_test=test_metrics,      # from SemEval test processing\n",
    "    inspec_train=train_metrics,      # from Inspec train processing\n",
    "    inspec_test=test_metrics        # from Inspec test processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07e98a-82f6-4543-a293-2469d950d27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
